{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e8c21cb6-1d93-4356-962a-6523349f54b7",
    "_uuid": "c6d3d82b20ec1911d04a7fe511365f1e3b4fd8ae"
   },
   "source": [
    "![](https://mma.prnewswire.com/media/1429854/MoneyLion_Logo.jpg?p=facebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='8'>8. Modeling </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "38f8d351-bb91-4065-b8f8-02f5f8c546f3",
    "_uuid": "3fb32ad935e230321b76aaa9335b9e0b48cc942e"
   },
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = final_data.copy()\n",
    "X = data.drop(['Target'], axis=1)\n",
    "Xbest=X[embeded_lgb_feature] #best features\n",
    "target = data['Target']\n",
    "# split into train and test\n",
    "#stratify = target means that the : returns training and test subsets that have the same proportions of class labels.\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xbest, target, test_size = 0.3, stratify = target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement KFold cross validation, we will use the LightGBM **cross validation** function, `cv`, because this allows us to use a critical technique for training a GBM, early stopping.  \n",
    "To use the cv function, we first need to make a LightGBM dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the cv function, we first need to make a LightGBM dataset.\n",
    "train_set = lgb.Dataset(X_train.values, label=y_train.values,\n",
    "                           feature_name=X_train.columns.tolist()\n",
    "                           )\n",
    "test_set =  lgb.Dataset(X_valid.values, label=y_valid.values,\n",
    "                       feature_name=X_train.columns.tolist()\n",
    "                       )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cv call, the num_boost_round is set to 10,000 (num_boost_round is the same as n_estimators), but this number won't actually be reached because we are using early stopping.\n",
    "\n",
    "The code below carries out both cross validation with 5 folds and early stopping with 100 early stopping rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get default hyperparameters\n",
    "model = lgb.LGBMClassifier()\n",
    "default_params = model.get_params()\n",
    "\n",
    "# Remove the number of estimators because we set this to 10000 in the cv call\n",
    "del default_params['n_estimators']\n",
    "\n",
    "# Cross validation with early stopping\n",
    "cv_results = lgb.cv(default_params, train_set, num_boost_round = 10000, early_stopping_rounds = 100, \n",
    "                    metrics = 'auc', nfold = 5, seed = 42)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# Optimal number of esimators found in cv\n",
    "model.n_estimators = len(cv_results['auc-mean'])\n",
    "\n",
    "# Train and make predicions with model\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict_proba(X_test)[:, 1]\n",
    "baseline_auc = roc_auc_score(y_test, preds)\n",
    "\n",
    "# print('The baseline model scores {:.5f} ROC AUC on the test set.'.format(baseline_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective Function**  \n",
    "\n",
    "The objective function takes in hyperparameters and outputs a value representing a score.Here our score will be the `ROC AUC` which of course we want to maximize. Later,we will have to use a value to minimize, so we can take  `1âˆ’ROC AUC`  as the score.  we will use cross validation with the specified model hyperparameters to get the cross-validation ROC AUC. This score will then be used to select the best model hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(hyperparameters, iteration):\n",
    "    \"\"\"Objective function for grid search. Returns\n",
    "       the cross validation score from a set of hyperparameters.\"\"\"\n",
    "    \n",
    "    # Number of estimators will be found using early stopping\n",
    "    if 'n_estimators' in hyperparameters.keys():\n",
    "        del hyperparameters['n_estimators']\n",
    "    \n",
    "     # Perform n_folds cross validation\n",
    "    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold = 5, \n",
    "                        early_stopping_rounds = 100, metrics = 'auc', seed = 42)\n",
    "    \n",
    "    # results to retun\n",
    "    score = cv_results['auc-mean'][-1]\n",
    "    estimators = len(cv_results['auc-mean'])\n",
    "    hyperparameters['n_estimators'] = estimators \n",
    "    \n",
    "    return [score, hyperparameters, iteration]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002882 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1015\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 19\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002100 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1015\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 19\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000934 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1015\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000941 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1015\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001216 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1015\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Warning] Unknown parameter: importance_type\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score 0.008338\n",
      "[LightGBM] [Info] Start training from score 0.008338\n",
      "[LightGBM] [Info] Start training from score 0.008383\n",
      "[LightGBM] [Info] Start training from score 0.008383\n",
      "[LightGBM] [Info] Start training from score 0.008337\n",
      "The cross-validation ROC AUC was 0.76715.\n"
     ]
    }
   ],
   "source": [
    "score, params, iteration = objective(default_params, 1)\n",
    "\n",
    "print('The cross-validation ROC AUC was {:.5f}.'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning Implementation\n",
    "we will use cross validation to determine the performance of model hyperparameters and early stopping with the GBM so we do not have to tune the number of estimators. The basic strategy for both grid and random search is simple: for each hyperparameter value combination, evaluate the cross validation score and record the results along with the hyperparameters. Then, at the end of searching, choose the hyperparameters that yielded the highest cross-validation score, train the model on all the training data, and make predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'class_weight': None,\n",
       " 'colsample_bytree': 1.0,\n",
       " 'importance_type': 'split',\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': -1,\n",
       " 'min_child_samples': 20,\n",
       " 'min_child_weight': 0.001,\n",
       " 'min_split_gain': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': -1,\n",
       " 'num_leaves': 31,\n",
       " 'objective': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': 0.0,\n",
       " 'reg_lambda': 0.0,\n",
       " 'silent': 'warn',\n",
       " 'subsample': 1.0,\n",
       " 'subsample_for_bin': 200000,\n",
       " 'subsample_freq': 0}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a default model\n",
    "model = lgb.LGBMModel()\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these we do not need to tune such as `silent`, `objective`, `random_state`, and `n_jobs`, and we will use early stopping to determine perhaps the most important hyperparameter, the number of individual learners trained, `n_estimators` (also referred to as `num_boost_rounds` or the number of iterations). Some of the hyperparameters do not need to be tuned if others are: for example, `min_child_samples` and `min_child_weight` both limit the complexity of individual decision trees by adjusting the minimum leaf observation requirements and therefore we will only adjust one. However, there are still many hyperparameters to optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the values in the dicionary must be a list, so we use list combined with range, np.linspace, and np.logspace to define the range of values for each hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'boosting_type': ['gbdt', 'goss'],\n",
    "    'num_leaves': list(range(20, 100)),\n",
    "    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 100)),\n",
    "    'subsample_for_bin': list(range(20000, 30000, 200)),\n",
    "    'min_child_samples': list(range(20, 500, 5)),\n",
    "    'reg_alpha': list(np.linspace(0, 1)),\n",
    "    'reg_lambda': list(np.linspace(0, 1)),\n",
    "    'colsample_bytree': list(np.linspace(0.6, 1, 10)),\n",
    "    'subsample': list(np.linspace(0.5, 1, 10))\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000883 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000676 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000715 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000689 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000790 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 0.008338\n",
      "[LightGBM] [Info] Start training from score 0.008338\n",
      "[LightGBM] [Info] Start training from score 0.008383\n",
      "[LightGBM] [Info] Start training from score 0.008383\n",
      "[LightGBM] [Info] Start training from score 0.008337\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000602 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000690 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000729 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000704 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 0.008338\n",
      "[LightGBM] [Info] Start training from score 0.008338\n",
      "[LightGBM] [Info] Start training from score 0.008383\n",
      "[LightGBM] [Info] Start training from score 0.008383\n",
      "[LightGBM] [Info] Start training from score 0.008337\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001007 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000625 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000667 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 0.008338\n",
      "[LightGBM] [Info] Start training from score 0.008338\n",
      "[LightGBM] [Info] Start training from score 0.008383\n",
      "[LightGBM] [Info] Start training from score 0.008383\n",
      "[LightGBM] [Info] Start training from score 0.008337\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000823 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000903 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000688 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001169 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 0.008338\n",
      "[LightGBM] [Info] Start training from score 0.008338\n",
      "[LightGBM] [Info] Start training from score 0.008383\n",
      "[LightGBM] [Info] Start training from score 0.008383\n",
      "[LightGBM] [Info] Start training from score 0.008337\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000871 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000874 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000854 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000652 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000581 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 0.008338\n",
      "[LightGBM] [Info] Start training from score 0.008338\n",
      "[LightGBM] [Info] Start training from score 0.008383\n",
      "[LightGBM] [Info] Start training from score 0.008383\n",
      "[LightGBM] [Info] Start training from score 0.008337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001844 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001588 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001663 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001890 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 0.008338\n",
      "[LightGBM] [Info] Start training from score 0.008338\n",
      "[LightGBM] [Info] Start training from score 0.008383\n",
      "[LightGBM] [Info] Start training from score 0.008383\n",
      "[LightGBM] [Info] Start training from score 0.008337\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000601 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000874 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000854 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000846 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 0.008338\n",
      "[LightGBM] [Info] Start training from score 0.008338\n",
      "[LightGBM] [Info] Start training from score 0.008383\n",
      "[LightGBM] [Info] Start training from score 0.008383\n",
      "[LightGBM] [Info] Start training from score 0.008337\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000766 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000832 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000622 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 0.008338\n",
      "[LightGBM] [Info] Start training from score 0.008338\n",
      "[LightGBM] [Info] Start training from score 0.008383\n",
      "[LightGBM] [Info] Start training from score 0.008383\n",
      "[LightGBM] [Info] Start training from score 0.008337\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000925 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000879 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000818 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000904 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000963 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 0.008338\n",
      "[LightGBM] [Info] Start training from score 0.008338\n",
      "[LightGBM] [Info] Start training from score 0.008383\n",
      "[LightGBM] [Info] Start training from score 0.008383\n",
      "[LightGBM] [Info] Start training from score 0.008337\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002301 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001517 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001448 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001393 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 0.008338\n",
      "[LightGBM] [Info] Start training from score 0.008338\n",
      "[LightGBM] [Info] Start training from score 0.008383\n",
      "[LightGBM] [Info] Start training from score 0.008383\n",
      "[LightGBM] [Info] Start training from score 0.008337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21829, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002950 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002763 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000992 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 21830, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 0.008338\n",
      "[LightGBM] [Info] Start training from score 0.008338\n",
      "[LightGBM] [Info] Start training from score 0.008383\n",
      "[LightGBM] [Info] Start training from score 0.008383\n",
      "[LightGBM] [Info] Start training from score 0.008337\n",
      "The best validation score was 0.80351\n",
      "\n",
      "The best hyperparameters were:\n",
      "{'boosting_type': 'gbdt',\n",
      " 'colsample_bytree': 0.6444444444444444,\n",
      " 'learning_rate': 0.004999999999999999,\n",
      " 'min_child_samples': 20,\n",
      " 'n_estimators': 631,\n",
      " 'num_leaves': 20,\n",
      " 'reg_alpha': 0.0,\n",
      " 'reg_lambda': 0.0,\n",
      " 'subsample': 0.5,\n",
      " 'subsample_for_bin': 20000}\n"
     ]
    }
   ],
   "source": [
    "def grid_search(param_grid, MAX_EVALS = 5):\n",
    "    \"\"\"Grid search algorithm (with limit on max evals)\"\"\"\n",
    "    \n",
    "    # Dataframe to store results\n",
    "    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "                              index = list(range(MAX_EVALS)))\n",
    "    \n",
    "    keys, values = zip(*param_grid.items()) # unpack the values in the hyperparameter grid dictionary\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    # Iterate through every possible combination of hyperparameters\n",
    "    for v in itertools.product(*values):\n",
    "        \n",
    "        # Create a hyperparameter dictionary\n",
    "        hyperparameters = dict(zip(keys, v))\n",
    "        \n",
    "        # Set the subsample ratio accounting for boosting type\n",
    "        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n",
    "        \n",
    "        #The objective function returns the cross validation score from the hyperparameters which we record in the dataframe.\n",
    "        eval_results = objective(hyperparameters, i)\n",
    "        \n",
    "        results.loc[i, :] = eval_results\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        # Here we will run grid search for 5 iterations for limited time\n",
    "        if i > MAX_EVALS:\n",
    "            break\n",
    "       \n",
    "    # Sort with best score on top\n",
    "    results.sort_values('score', ascending = False, inplace = True)\n",
    "    results.reset_index(inplace = True)\n",
    "    \n",
    "    return results   \n",
    "\n",
    "grid_results = grid_search(param_grid)\n",
    "\n",
    "print('The best validation score was {:.5f}'.format(grid_results.loc[0, 'score']))\n",
    "print('\\nThe best hyperparameters were:')\n",
    "\n",
    "pprint.pprint(grid_results.loc[0, 'params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "'boosting_type': 'gbdt',\n",
    " 'colsample_bytree': 0.6,\n",
    " 'learning_rate': 0.004999999999999999,\n",
    " 'min_child_samples': 20,\n",
    " 'n_estimators': 94,\n",
    " 'num_leaves': 20,\n",
    " 'objective': 'binary',\n",
    " 'save_binary': True,\n",
    " 'verbose': 1,\n",
    " 'n_estimators': 1000,\n",
    " 'metric': 'auc',\n",
    " 'is_unbalance': True,\n",
    " 'reg_alpha': 0.0,\n",
    " 'reg_lambda': 0.0,\n",
    " 'subsample': 0.5,\n",
    " 'subsample_for_bin': 20000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 228, number of negative: 27059\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 995\n",
      "[LightGBM] [Info] Number of data points in the train set: 27287, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.008356 -> initscore=-4.776429\n",
      "[LightGBM] [Info] Start training from score -4.776429\n",
      "[1]\ttraining's auc: 0.832723\tvalid_1's auc: 0.810064\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[2]\ttraining's auc: 0.863834\tvalid_1's auc: 0.83701\n",
      "[3]\ttraining's auc: 0.875344\tvalid_1's auc: 0.848847\n",
      "[4]\ttraining's auc: 0.883197\tvalid_1's auc: 0.853995\n",
      "[5]\ttraining's auc: 0.890316\tvalid_1's auc: 0.862422\n",
      "[6]\ttraining's auc: 0.896771\tvalid_1's auc: 0.865076\n",
      "[7]\ttraining's auc: 0.899682\tvalid_1's auc: 0.868088\n",
      "[8]\ttraining's auc: 0.901516\tvalid_1's auc: 0.869421\n",
      "[9]\ttraining's auc: 0.903958\tvalid_1's auc: 0.872009\n",
      "[10]\ttraining's auc: 0.905055\tvalid_1's auc: 0.873228\n",
      "[11]\ttraining's auc: 0.906881\tvalid_1's auc: 0.876023\n",
      "[12]\ttraining's auc: 0.907623\tvalid_1's auc: 0.875812\n",
      "[13]\ttraining's auc: 0.908444\tvalid_1's auc: 0.876961\n",
      "[14]\ttraining's auc: 0.909651\tvalid_1's auc: 0.879045\n",
      "[15]\ttraining's auc: 0.911233\tvalid_1's auc: 0.878613\n",
      "[16]\ttraining's auc: 0.911967\tvalid_1's auc: 0.879073\n",
      "[17]\ttraining's auc: 0.911845\tvalid_1's auc: 0.879214\n",
      "[18]\ttraining's auc: 0.911931\tvalid_1's auc: 0.8789\n",
      "[19]\ttraining's auc: 0.912747\tvalid_1's auc: 0.879854\n",
      "[20]\ttraining's auc: 0.913184\tvalid_1's auc: 0.88022\n",
      "[21]\ttraining's auc: 0.913767\tvalid_1's auc: 0.880542\n",
      "[22]\ttraining's auc: 0.914538\tvalid_1's auc: 0.881632\n",
      "[23]\ttraining's auc: 0.914961\tvalid_1's auc: 0.882067\n",
      "[24]\ttraining's auc: 0.915017\tvalid_1's auc: 0.882223\n",
      "[25]\ttraining's auc: 0.91588\tvalid_1's auc: 0.882979\n",
      "[26]\ttraining's auc: 0.916488\tvalid_1's auc: 0.88324\n",
      "[27]\ttraining's auc: 0.917491\tvalid_1's auc: 0.883992\n",
      "[28]\ttraining's auc: 0.919297\tvalid_1's auc: 0.885729\n",
      "[29]\ttraining's auc: 0.920046\tvalid_1's auc: 0.88597\n",
      "[30]\ttraining's auc: 0.92113\tvalid_1's auc: 0.886836\n",
      "[31]\ttraining's auc: 0.921491\tvalid_1's auc: 0.887035\n",
      "[32]\ttraining's auc: 0.921852\tvalid_1's auc: 0.887175\n",
      "[33]\ttraining's auc: 0.92203\tvalid_1's auc: 0.887358\n",
      "[34]\ttraining's auc: 0.922998\tvalid_1's auc: 0.887598\n",
      "[35]\ttraining's auc: 0.923242\tvalid_1's auc: 0.888049\n",
      "[36]\ttraining's auc: 0.923607\tvalid_1's auc: 0.888433\n",
      "[37]\ttraining's auc: 0.925025\tvalid_1's auc: 0.889893\n",
      "[38]\ttraining's auc: 0.92648\tvalid_1's auc: 0.891125\n",
      "[39]\ttraining's auc: 0.926884\tvalid_1's auc: 0.891414\n",
      "[40]\ttraining's auc: 0.927654\tvalid_1's auc: 0.89208\n",
      "[41]\ttraining's auc: 0.927913\tvalid_1's auc: 0.892167\n",
      "[42]\ttraining's auc: 0.928775\tvalid_1's auc: 0.892409\n",
      "[43]\ttraining's auc: 0.929025\tvalid_1's auc: 0.892507\n",
      "[44]\ttraining's auc: 0.929205\tvalid_1's auc: 0.892904\n",
      "[45]\ttraining's auc: 0.929182\tvalid_1's auc: 0.892839\n",
      "[46]\ttraining's auc: 0.92995\tvalid_1's auc: 0.893727\n",
      "[47]\ttraining's auc: 0.930647\tvalid_1's auc: 0.894322\n",
      "[48]\ttraining's auc: 0.93105\tvalid_1's auc: 0.894725\n",
      "[49]\ttraining's auc: 0.931173\tvalid_1's auc: 0.894939\n",
      "[50]\ttraining's auc: 0.931345\tvalid_1's auc: 0.895069\n",
      "[51]\ttraining's auc: 0.93173\tvalid_1's auc: 0.895209\n",
      "[52]\ttraining's auc: 0.931991\tvalid_1's auc: 0.895596\n",
      "[53]\ttraining's auc: 0.932193\tvalid_1's auc: 0.89584\n",
      "[54]\ttraining's auc: 0.932306\tvalid_1's auc: 0.896048\n",
      "[55]\ttraining's auc: 0.933827\tvalid_1's auc: 0.897362\n",
      "[56]\ttraining's auc: 0.934401\tvalid_1's auc: 0.897858\n",
      "[57]\ttraining's auc: 0.934548\tvalid_1's auc: 0.897782\n",
      "[58]\ttraining's auc: 0.93465\tvalid_1's auc: 0.897595\n",
      "[59]\ttraining's auc: 0.934936\tvalid_1's auc: 0.89776\n",
      "[60]\ttraining's auc: 0.935219\tvalid_1's auc: 0.897927\n",
      "[61]\ttraining's auc: 0.935467\tvalid_1's auc: 0.898439\n",
      "[62]\ttraining's auc: 0.935564\tvalid_1's auc: 0.89862\n",
      "[63]\ttraining's auc: 0.935731\tvalid_1's auc: 0.898805\n",
      "[64]\ttraining's auc: 0.935959\tvalid_1's auc: 0.89916\n",
      "[65]\ttraining's auc: 0.937684\tvalid_1's auc: 0.900217\n",
      "[66]\ttraining's auc: 0.9377\tvalid_1's auc: 0.900465\n",
      "[67]\ttraining's auc: 0.937816\tvalid_1's auc: 0.900595\n",
      "[68]\ttraining's auc: 0.937811\tvalid_1's auc: 0.900718\n",
      "[69]\ttraining's auc: 0.937975\tvalid_1's auc: 0.90096\n",
      "[70]\ttraining's auc: 0.937932\tvalid_1's auc: 0.900992\n",
      "[71]\ttraining's auc: 0.938145\tvalid_1's auc: 0.901166\n",
      "[72]\ttraining's auc: 0.938275\tvalid_1's auc: 0.901338\n",
      "[73]\ttraining's auc: 0.938316\tvalid_1's auc: 0.901439\n",
      "[74]\ttraining's auc: 0.938328\tvalid_1's auc: 0.90146\n",
      "[75]\ttraining's auc: 0.938213\tvalid_1's auc: 0.901459\n",
      "[76]\ttraining's auc: 0.938286\tvalid_1's auc: 0.901602\n",
      "[77]\ttraining's auc: 0.938491\tvalid_1's auc: 0.901694\n",
      "[78]\ttraining's auc: 0.938693\tvalid_1's auc: 0.902004\n",
      "[79]\ttraining's auc: 0.938689\tvalid_1's auc: 0.901896\n",
      "[80]\ttraining's auc: 0.938948\tvalid_1's auc: 0.90214\n",
      "[81]\ttraining's auc: 0.93908\tvalid_1's auc: 0.902202\n",
      "[82]\ttraining's auc: 0.939475\tvalid_1's auc: 0.902437\n",
      "[83]\ttraining's auc: 0.94\tvalid_1's auc: 0.902965\n",
      "[84]\ttraining's auc: 0.939856\tvalid_1's auc: 0.902907\n",
      "[85]\ttraining's auc: 0.940052\tvalid_1's auc: 0.903112\n",
      "[86]\ttraining's auc: 0.940071\tvalid_1's auc: 0.903159\n",
      "[87]\ttraining's auc: 0.940103\tvalid_1's auc: 0.90317\n",
      "[88]\ttraining's auc: 0.94059\tvalid_1's auc: 0.903609\n",
      "[89]\ttraining's auc: 0.940909\tvalid_1's auc: 0.903737\n",
      "[90]\ttraining's auc: 0.94109\tvalid_1's auc: 0.903904\n",
      "[91]\ttraining's auc: 0.941084\tvalid_1's auc: 0.90399\n",
      "[92]\ttraining's auc: 0.941283\tvalid_1's auc: 0.90416\n",
      "[93]\ttraining's auc: 0.94136\tvalid_1's auc: 0.904203\n",
      "[94]\ttraining's auc: 0.941799\tvalid_1's auc: 0.904583\n",
      "[95]\ttraining's auc: 0.942157\tvalid_1's auc: 0.904586\n",
      "[96]\ttraining's auc: 0.942692\tvalid_1's auc: 0.905103\n",
      "[97]\ttraining's auc: 0.943029\tvalid_1's auc: 0.905331\n",
      "[98]\ttraining's auc: 0.943452\tvalid_1's auc: 0.905778\n",
      "[99]\ttraining's auc: 0.943512\tvalid_1's auc: 0.905883\n",
      "[100]\ttraining's auc: 0.943676\tvalid_1's auc: 0.906026\n",
      "[101]\ttraining's auc: 0.943809\tvalid_1's auc: 0.906208\n",
      "[102]\ttraining's auc: 0.943882\tvalid_1's auc: 0.906209\n",
      "[103]\ttraining's auc: 0.943953\tvalid_1's auc: 0.906138\n",
      "[104]\ttraining's auc: 0.944353\tvalid_1's auc: 0.906416\n",
      "[105]\ttraining's auc: 0.944441\tvalid_1's auc: 0.906578\n",
      "[106]\ttraining's auc: 0.944561\tvalid_1's auc: 0.906677\n",
      "[107]\ttraining's auc: 0.944778\tvalid_1's auc: 0.906797\n",
      "[108]\ttraining's auc: 0.945591\tvalid_1's auc: 0.907473\n",
      "[109]\ttraining's auc: 0.945813\tvalid_1's auc: 0.90763\n",
      "[110]\ttraining's auc: 0.945899\tvalid_1's auc: 0.907693\n",
      "[111]\ttraining's auc: 0.945981\tvalid_1's auc: 0.907869\n",
      "[112]\ttraining's auc: 0.946114\tvalid_1's auc: 0.907955\n",
      "[113]\ttraining's auc: 0.946119\tvalid_1's auc: 0.90803\n",
      "[114]\ttraining's auc: 0.946076\tvalid_1's auc: 0.908038\n",
      "[115]\ttraining's auc: 0.946544\tvalid_1's auc: 0.908448\n",
      "[116]\ttraining's auc: 0.946501\tvalid_1's auc: 0.908351\n",
      "[117]\ttraining's auc: 0.946483\tvalid_1's auc: 0.908367\n",
      "[118]\ttraining's auc: 0.94641\tvalid_1's auc: 0.908337\n",
      "[119]\ttraining's auc: 0.946446\tvalid_1's auc: 0.908309\n",
      "[120]\ttraining's auc: 0.946537\tvalid_1's auc: 0.908354\n",
      "[121]\ttraining's auc: 0.946736\tvalid_1's auc: 0.908607\n",
      "[122]\ttraining's auc: 0.946984\tvalid_1's auc: 0.908784\n",
      "[123]\ttraining's auc: 0.947231\tvalid_1's auc: 0.908919\n",
      "[124]\ttraining's auc: 0.947303\tvalid_1's auc: 0.908912\n",
      "[125]\ttraining's auc: 0.947463\tvalid_1's auc: 0.909081\n",
      "[126]\ttraining's auc: 0.947647\tvalid_1's auc: 0.909137\n",
      "[127]\ttraining's auc: 0.947618\tvalid_1's auc: 0.909069\n",
      "[128]\ttraining's auc: 0.947681\tvalid_1's auc: 0.909104\n",
      "[129]\ttraining's auc: 0.948036\tvalid_1's auc: 0.909244\n",
      "[130]\ttraining's auc: 0.948388\tvalid_1's auc: 0.909393\n",
      "[131]\ttraining's auc: 0.948461\tvalid_1's auc: 0.909385\n",
      "[132]\ttraining's auc: 0.948615\tvalid_1's auc: 0.909426\n",
      "[133]\ttraining's auc: 0.948686\tvalid_1's auc: 0.90942\n",
      "[134]\ttraining's auc: 0.94885\tvalid_1's auc: 0.909484\n",
      "[135]\ttraining's auc: 0.948908\tvalid_1's auc: 0.909582\n",
      "[136]\ttraining's auc: 0.948958\tvalid_1's auc: 0.909707\n",
      "[137]\ttraining's auc: 0.949026\tvalid_1's auc: 0.909699\n",
      "[138]\ttraining's auc: 0.949148\tvalid_1's auc: 0.909677\n",
      "[139]\ttraining's auc: 0.949219\tvalid_1's auc: 0.909733\n",
      "[140]\ttraining's auc: 0.949198\tvalid_1's auc: 0.909753\n",
      "[141]\ttraining's auc: 0.949379\tvalid_1's auc: 0.909929\n",
      "[142]\ttraining's auc: 0.949442\tvalid_1's auc: 0.910019\n",
      "[143]\ttraining's auc: 0.949415\tvalid_1's auc: 0.910044\n",
      "[144]\ttraining's auc: 0.949539\tvalid_1's auc: 0.910258\n",
      "[145]\ttraining's auc: 0.949548\tvalid_1's auc: 0.910297\n",
      "[146]\ttraining's auc: 0.949504\tvalid_1's auc: 0.910187\n",
      "[147]\ttraining's auc: 0.949454\tvalid_1's auc: 0.910115\n",
      "[148]\ttraining's auc: 0.949433\tvalid_1's auc: 0.910186\n",
      "[149]\ttraining's auc: 0.949363\tvalid_1's auc: 0.910108\n",
      "[150]\ttraining's auc: 0.949362\tvalid_1's auc: 0.910076\n",
      "[151]\ttraining's auc: 0.949422\tvalid_1's auc: 0.910088\n",
      "[152]\ttraining's auc: 0.949528\tvalid_1's auc: 0.910247\n",
      "[153]\ttraining's auc: 0.949584\tvalid_1's auc: 0.910244\n",
      "[154]\ttraining's auc: 0.950174\tvalid_1's auc: 0.910616\n",
      "[155]\ttraining's auc: 0.950483\tvalid_1's auc: 0.910967\n",
      "[156]\ttraining's auc: 0.951089\tvalid_1's auc: 0.911374\n",
      "[157]\ttraining's auc: 0.951617\tvalid_1's auc: 0.911613\n",
      "[158]\ttraining's auc: 0.951974\tvalid_1's auc: 0.911887\n",
      "[159]\ttraining's auc: 0.952039\tvalid_1's auc: 0.911902\n",
      "[160]\ttraining's auc: 0.952172\tvalid_1's auc: 0.912098\n",
      "[161]\ttraining's auc: 0.952289\tvalid_1's auc: 0.912215\n",
      "[162]\ttraining's auc: 0.952433\tvalid_1's auc: 0.912179\n",
      "[163]\ttraining's auc: 0.952487\tvalid_1's auc: 0.912231\n",
      "[164]\ttraining's auc: 0.952413\tvalid_1's auc: 0.912208\n",
      "[165]\ttraining's auc: 0.952409\tvalid_1's auc: 0.912249\n",
      "[166]\ttraining's auc: 0.952457\tvalid_1's auc: 0.912306\n",
      "[167]\ttraining's auc: 0.952481\tvalid_1's auc: 0.912249\n",
      "[168]\ttraining's auc: 0.952527\tvalid_1's auc: 0.912334\n",
      "[169]\ttraining's auc: 0.952604\tvalid_1's auc: 0.912372\n",
      "[170]\ttraining's auc: 0.952676\tvalid_1's auc: 0.912462\n",
      "[171]\ttraining's auc: 0.953053\tvalid_1's auc: 0.912705\n",
      "[172]\ttraining's auc: 0.953409\tvalid_1's auc: 0.912927\n",
      "[173]\ttraining's auc: 0.953467\tvalid_1's auc: 0.912907\n",
      "[174]\ttraining's auc: 0.953488\tvalid_1's auc: 0.912897\n",
      "[175]\ttraining's auc: 0.953513\tvalid_1's auc: 0.912962\n",
      "[176]\ttraining's auc: 0.954025\tvalid_1's auc: 0.913263\n",
      "[177]\ttraining's auc: 0.954082\tvalid_1's auc: 0.913235\n",
      "[178]\ttraining's auc: 0.954411\tvalid_1's auc: 0.913426\n",
      "[179]\ttraining's auc: 0.954441\tvalid_1's auc: 0.913493\n",
      "[180]\ttraining's auc: 0.954444\tvalid_1's auc: 0.913495\n",
      "[181]\ttraining's auc: 0.954477\tvalid_1's auc: 0.913531\n",
      "[182]\ttraining's auc: 0.954566\tvalid_1's auc: 0.913558\n",
      "[183]\ttraining's auc: 0.954634\tvalid_1's auc: 0.913603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[184]\ttraining's auc: 0.954669\tvalid_1's auc: 0.913627\n",
      "[185]\ttraining's auc: 0.954721\tvalid_1's auc: 0.913613\n",
      "[186]\ttraining's auc: 0.954668\tvalid_1's auc: 0.913548\n",
      "[187]\ttraining's auc: 0.954833\tvalid_1's auc: 0.913548\n",
      "[188]\ttraining's auc: 0.954889\tvalid_1's auc: 0.913659\n",
      "[189]\ttraining's auc: 0.954816\tvalid_1's auc: 0.913524\n",
      "[190]\ttraining's auc: 0.95476\tvalid_1's auc: 0.913522\n",
      "[191]\ttraining's auc: 0.955244\tvalid_1's auc: 0.913714\n",
      "[192]\ttraining's auc: 0.955284\tvalid_1's auc: 0.913751\n",
      "[193]\ttraining's auc: 0.955272\tvalid_1's auc: 0.913777\n",
      "[194]\ttraining's auc: 0.955649\tvalid_1's auc: 0.914007\n",
      "[195]\ttraining's auc: 0.955904\tvalid_1's auc: 0.914229\n",
      "[196]\ttraining's auc: 0.956011\tvalid_1's auc: 0.914213\n",
      "[197]\ttraining's auc: 0.955966\tvalid_1's auc: 0.914218\n",
      "[198]\ttraining's auc: 0.956004\tvalid_1's auc: 0.914308\n",
      "[199]\ttraining's auc: 0.956152\tvalid_1's auc: 0.914304\n",
      "[200]\ttraining's auc: 0.95647\tvalid_1's auc: 0.914465\n",
      "[201]\ttraining's auc: 0.95684\tvalid_1's auc: 0.914673\n",
      "[202]\ttraining's auc: 0.95679\tvalid_1's auc: 0.914684\n",
      "[203]\ttraining's auc: 0.95685\tvalid_1's auc: 0.914718\n",
      "[204]\ttraining's auc: 0.956824\tvalid_1's auc: 0.914777\n",
      "[205]\ttraining's auc: 0.957089\tvalid_1's auc: 0.914911\n",
      "[206]\ttraining's auc: 0.957103\tvalid_1's auc: 0.914927\n",
      "[207]\ttraining's auc: 0.957196\tvalid_1's auc: 0.914975\n",
      "[208]\ttraining's auc: 0.957219\tvalid_1's auc: 0.914994\n",
      "[209]\ttraining's auc: 0.957447\tvalid_1's auc: 0.91517\n",
      "[210]\ttraining's auc: 0.957475\tvalid_1's auc: 0.915212\n",
      "[211]\ttraining's auc: 0.957433\tvalid_1's auc: 0.91516\n",
      "[212]\ttraining's auc: 0.957489\tvalid_1's auc: 0.915217\n",
      "[213]\ttraining's auc: 0.957563\tvalid_1's auc: 0.915298\n",
      "[214]\ttraining's auc: 0.95771\tvalid_1's auc: 0.915445\n",
      "[215]\ttraining's auc: 0.957624\tvalid_1's auc: 0.915425\n",
      "[216]\ttraining's auc: 0.957542\tvalid_1's auc: 0.915468\n",
      "[217]\ttraining's auc: 0.95778\tvalid_1's auc: 0.915699\n",
      "[218]\ttraining's auc: 0.95782\tvalid_1's auc: 0.915681\n",
      "[219]\ttraining's auc: 0.95773\tvalid_1's auc: 0.915634\n",
      "[220]\ttraining's auc: 0.957749\tvalid_1's auc: 0.915634\n",
      "[221]\ttraining's auc: 0.957799\tvalid_1's auc: 0.915519\n",
      "[222]\ttraining's auc: 0.957998\tvalid_1's auc: 0.915735\n",
      "[223]\ttraining's auc: 0.957968\tvalid_1's auc: 0.915753\n",
      "[224]\ttraining's auc: 0.958146\tvalid_1's auc: 0.915918\n",
      "[225]\ttraining's auc: 0.958222\tvalid_1's auc: 0.916032\n",
      "[226]\ttraining's auc: 0.958266\tvalid_1's auc: 0.916032\n",
      "[227]\ttraining's auc: 0.9585\tvalid_1's auc: 0.916087\n",
      "[228]\ttraining's auc: 0.958511\tvalid_1's auc: 0.916147\n",
      "[229]\ttraining's auc: 0.958742\tvalid_1's auc: 0.916282\n",
      "[230]\ttraining's auc: 0.958799\tvalid_1's auc: 0.916282\n",
      "[231]\ttraining's auc: 0.958826\tvalid_1's auc: 0.916386\n",
      "[232]\ttraining's auc: 0.958761\tvalid_1's auc: 0.916397\n",
      "[233]\ttraining's auc: 0.958682\tvalid_1's auc: 0.916348\n",
      "[234]\ttraining's auc: 0.95863\tvalid_1's auc: 0.916371\n",
      "[235]\ttraining's auc: 0.958669\tvalid_1's auc: 0.916356\n",
      "[236]\ttraining's auc: 0.958957\tvalid_1's auc: 0.916483\n",
      "[237]\ttraining's auc: 0.959001\tvalid_1's auc: 0.916562\n",
      "[238]\ttraining's auc: 0.959044\tvalid_1's auc: 0.916586\n",
      "[239]\ttraining's auc: 0.958975\tvalid_1's auc: 0.916535\n",
      "[240]\ttraining's auc: 0.958938\tvalid_1's auc: 0.916514\n",
      "[241]\ttraining's auc: 0.958888\tvalid_1's auc: 0.916495\n",
      "[242]\ttraining's auc: 0.958875\tvalid_1's auc: 0.916515\n",
      "[243]\ttraining's auc: 0.959162\tvalid_1's auc: 0.916698\n",
      "[244]\ttraining's auc: 0.959373\tvalid_1's auc: 0.916736\n",
      "[245]\ttraining's auc: 0.959374\tvalid_1's auc: 0.916749\n",
      "[246]\ttraining's auc: 0.959711\tvalid_1's auc: 0.916949\n",
      "[247]\ttraining's auc: 0.959743\tvalid_1's auc: 0.916927\n",
      "[248]\ttraining's auc: 0.959762\tvalid_1's auc: 0.916975\n",
      "[249]\ttraining's auc: 0.959812\tvalid_1's auc: 0.91704\n",
      "[250]\ttraining's auc: 0.959782\tvalid_1's auc: 0.91701\n",
      "[251]\ttraining's auc: 0.959783\tvalid_1's auc: 0.917099\n",
      "[252]\ttraining's auc: 0.959919\tvalid_1's auc: 0.917232\n",
      "[253]\ttraining's auc: 0.959875\tvalid_1's auc: 0.917206\n",
      "[254]\ttraining's auc: 0.960216\tvalid_1's auc: 0.917393\n",
      "[255]\ttraining's auc: 0.96017\tvalid_1's auc: 0.91742\n",
      "[256]\ttraining's auc: 0.960309\tvalid_1's auc: 0.917402\n",
      "[257]\ttraining's auc: 0.960269\tvalid_1's auc: 0.917374\n",
      "[258]\ttraining's auc: 0.960225\tvalid_1's auc: 0.917368\n",
      "[259]\ttraining's auc: 0.960191\tvalid_1's auc: 0.917315\n",
      "[260]\ttraining's auc: 0.960411\tvalid_1's auc: 0.917424\n",
      "[261]\ttraining's auc: 0.960471\tvalid_1's auc: 0.917433\n",
      "[262]\ttraining's auc: 0.960482\tvalid_1's auc: 0.917441\n",
      "[263]\ttraining's auc: 0.960455\tvalid_1's auc: 0.917449\n",
      "[264]\ttraining's auc: 0.960803\tvalid_1's auc: 0.91758\n",
      "[265]\ttraining's auc: 0.960828\tvalid_1's auc: 0.917617\n",
      "[266]\ttraining's auc: 0.960928\tvalid_1's auc: 0.917617\n",
      "[267]\ttraining's auc: 0.961137\tvalid_1's auc: 0.917682\n",
      "[268]\ttraining's auc: 0.961306\tvalid_1's auc: 0.917733\n",
      "[269]\ttraining's auc: 0.96122\tvalid_1's auc: 0.917725\n",
      "[270]\ttraining's auc: 0.961238\tvalid_1's auc: 0.917752\n",
      "[271]\ttraining's auc: 0.9614\tvalid_1's auc: 0.917869\n",
      "[272]\ttraining's auc: 0.961428\tvalid_1's auc: 0.917884\n",
      "[273]\ttraining's auc: 0.961566\tvalid_1's auc: 0.917977\n",
      "[274]\ttraining's auc: 0.96161\tvalid_1's auc: 0.917946\n",
      "[275]\ttraining's auc: 0.961799\tvalid_1's auc: 0.917974\n",
      "[276]\ttraining's auc: 0.961829\tvalid_1's auc: 0.918037\n",
      "[277]\ttraining's auc: 0.962029\tvalid_1's auc: 0.918156\n",
      "[278]\ttraining's auc: 0.962091\tvalid_1's auc: 0.918163\n",
      "[279]\ttraining's auc: 0.962198\tvalid_1's auc: 0.918225\n",
      "[280]\ttraining's auc: 0.962292\tvalid_1's auc: 0.918305\n",
      "[281]\ttraining's auc: 0.96231\tvalid_1's auc: 0.918304\n",
      "[282]\ttraining's auc: 0.96241\tvalid_1's auc: 0.918349\n",
      "[283]\ttraining's auc: 0.962489\tvalid_1's auc: 0.918311\n",
      "[284]\ttraining's auc: 0.962672\tvalid_1's auc: 0.918325\n",
      "[285]\ttraining's auc: 0.962882\tvalid_1's auc: 0.918432\n",
      "[286]\ttraining's auc: 0.963259\tvalid_1's auc: 0.918678\n",
      "[287]\ttraining's auc: 0.963284\tvalid_1's auc: 0.918709\n",
      "[288]\ttraining's auc: 0.963288\tvalid_1's auc: 0.918765\n",
      "[289]\ttraining's auc: 0.963335\tvalid_1's auc: 0.918734\n",
      "[290]\ttraining's auc: 0.9633\tvalid_1's auc: 0.918712\n",
      "[291]\ttraining's auc: 0.963581\tvalid_1's auc: 0.918934\n",
      "[292]\ttraining's auc: 0.963719\tvalid_1's auc: 0.918948\n",
      "[293]\ttraining's auc: 0.963751\tvalid_1's auc: 0.918988\n",
      "[294]\ttraining's auc: 0.96406\tvalid_1's auc: 0.919101\n",
      "[295]\ttraining's auc: 0.964113\tvalid_1's auc: 0.919117\n",
      "[296]\ttraining's auc: 0.964059\tvalid_1's auc: 0.919156\n",
      "[297]\ttraining's auc: 0.964058\tvalid_1's auc: 0.919161\n",
      "[298]\ttraining's auc: 0.964096\tvalid_1's auc: 0.919141\n",
      "[299]\ttraining's auc: 0.964082\tvalid_1's auc: 0.919146\n",
      "[300]\ttraining's auc: 0.964143\tvalid_1's auc: 0.919239\n",
      "[301]\ttraining's auc: 0.964358\tvalid_1's auc: 0.919322\n",
      "[302]\ttraining's auc: 0.964354\tvalid_1's auc: 0.919306\n",
      "[303]\ttraining's auc: 0.964556\tvalid_1's auc: 0.919433\n",
      "[304]\ttraining's auc: 0.964559\tvalid_1's auc: 0.919417\n",
      "[305]\ttraining's auc: 0.964508\tvalid_1's auc: 0.919399\n",
      "[306]\ttraining's auc: 0.964533\tvalid_1's auc: 0.91939\n",
      "[307]\ttraining's auc: 0.964504\tvalid_1's auc: 0.919425\n",
      "[308]\ttraining's auc: 0.964558\tvalid_1's auc: 0.91943\n",
      "[309]\ttraining's auc: 0.964818\tvalid_1's auc: 0.919671\n",
      "[310]\ttraining's auc: 0.964837\tvalid_1's auc: 0.919653\n",
      "[311]\ttraining's auc: 0.965006\tvalid_1's auc: 0.919791\n",
      "[312]\ttraining's auc: 0.965006\tvalid_1's auc: 0.919787\n",
      "[313]\ttraining's auc: 0.964998\tvalid_1's auc: 0.919769\n",
      "[314]\ttraining's auc: 0.965134\tvalid_1's auc: 0.919875\n",
      "[315]\ttraining's auc: 0.965256\tvalid_1's auc: 0.919952\n",
      "[316]\ttraining's auc: 0.965386\tvalid_1's auc: 0.920011\n",
      "[317]\ttraining's auc: 0.965376\tvalid_1's auc: 0.920069\n",
      "[318]\ttraining's auc: 0.965479\tvalid_1's auc: 0.920139\n",
      "[319]\ttraining's auc: 0.965481\tvalid_1's auc: 0.920055\n",
      "[320]\ttraining's auc: 0.965462\tvalid_1's auc: 0.919948\n",
      "[321]\ttraining's auc: 0.965442\tvalid_1's auc: 0.919901\n",
      "[322]\ttraining's auc: 0.965484\tvalid_1's auc: 0.91991\n",
      "[323]\ttraining's auc: 0.965472\tvalid_1's auc: 0.919957\n",
      "[324]\ttraining's auc: 0.965544\tvalid_1's auc: 0.919991\n",
      "[325]\ttraining's auc: 0.965733\tvalid_1's auc: 0.920113\n",
      "[326]\ttraining's auc: 0.965899\tvalid_1's auc: 0.920265\n",
      "[327]\ttraining's auc: 0.96589\tvalid_1's auc: 0.920304\n",
      "[328]\ttraining's auc: 0.965902\tvalid_1's auc: 0.920262\n",
      "[329]\ttraining's auc: 0.965867\tvalid_1's auc: 0.920303\n",
      "[330]\ttraining's auc: 0.966039\tvalid_1's auc: 0.920407\n",
      "[331]\ttraining's auc: 0.966091\tvalid_1's auc: 0.920406\n",
      "[332]\ttraining's auc: 0.96627\tvalid_1's auc: 0.920521\n",
      "[333]\ttraining's auc: 0.966315\tvalid_1's auc: 0.920572\n",
      "[334]\ttraining's auc: 0.966327\tvalid_1's auc: 0.920618\n",
      "[335]\ttraining's auc: 0.966339\tvalid_1's auc: 0.920682\n",
      "[336]\ttraining's auc: 0.96639\tvalid_1's auc: 0.920691\n",
      "[337]\ttraining's auc: 0.966422\tvalid_1's auc: 0.920642\n",
      "[338]\ttraining's auc: 0.966414\tvalid_1's auc: 0.920672\n",
      "[339]\ttraining's auc: 0.966483\tvalid_1's auc: 0.920716\n",
      "[340]\ttraining's auc: 0.966623\tvalid_1's auc: 0.920828\n",
      "[341]\ttraining's auc: 0.966765\tvalid_1's auc: 0.920947\n",
      "[342]\ttraining's auc: 0.966824\tvalid_1's auc: 0.920881\n",
      "[343]\ttraining's auc: 0.967046\tvalid_1's auc: 0.921029\n",
      "[344]\ttraining's auc: 0.967123\tvalid_1's auc: 0.92112\n",
      "[345]\ttraining's auc: 0.967295\tvalid_1's auc: 0.92123\n",
      "[346]\ttraining's auc: 0.96725\tvalid_1's auc: 0.921175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[347]\ttraining's auc: 0.967357\tvalid_1's auc: 0.921223\n",
      "[348]\ttraining's auc: 0.967343\tvalid_1's auc: 0.921252\n",
      "[349]\ttraining's auc: 0.967374\tvalid_1's auc: 0.921222\n",
      "[350]\ttraining's auc: 0.967327\tvalid_1's auc: 0.921197\n",
      "[351]\ttraining's auc: 0.967476\tvalid_1's auc: 0.921359\n",
      "[352]\ttraining's auc: 0.967699\tvalid_1's auc: 0.921505\n",
      "[353]\ttraining's auc: 0.96783\tvalid_1's auc: 0.921601\n",
      "[354]\ttraining's auc: 0.967874\tvalid_1's auc: 0.921624\n",
      "[355]\ttraining's auc: 0.967908\tvalid_1's auc: 0.921621\n",
      "[356]\ttraining's auc: 0.967936\tvalid_1's auc: 0.921673\n",
      "[357]\ttraining's auc: 0.96816\tvalid_1's auc: 0.921802\n",
      "[358]\ttraining's auc: 0.96825\tvalid_1's auc: 0.921864\n",
      "[359]\ttraining's auc: 0.968323\tvalid_1's auc: 0.921928\n",
      "[360]\ttraining's auc: 0.968323\tvalid_1's auc: 0.921886\n",
      "[361]\ttraining's auc: 0.968576\tvalid_1's auc: 0.921991\n",
      "[362]\ttraining's auc: 0.968736\tvalid_1's auc: 0.922081\n",
      "[363]\ttraining's auc: 0.968832\tvalid_1's auc: 0.922195\n",
      "[364]\ttraining's auc: 0.969056\tvalid_1's auc: 0.922366\n",
      "[365]\ttraining's auc: 0.969204\tvalid_1's auc: 0.922475\n",
      "[366]\ttraining's auc: 0.969303\tvalid_1's auc: 0.922485\n",
      "[367]\ttraining's auc: 0.969435\tvalid_1's auc: 0.922576\n",
      "[368]\ttraining's auc: 0.969456\tvalid_1's auc: 0.922619\n",
      "[369]\ttraining's auc: 0.969459\tvalid_1's auc: 0.922561\n",
      "[370]\ttraining's auc: 0.969437\tvalid_1's auc: 0.922568\n",
      "[371]\ttraining's auc: 0.969447\tvalid_1's auc: 0.922633\n",
      "[372]\ttraining's auc: 0.969552\tvalid_1's auc: 0.922688\n",
      "[373]\ttraining's auc: 0.969661\tvalid_1's auc: 0.922747\n",
      "[374]\ttraining's auc: 0.969802\tvalid_1's auc: 0.922779\n",
      "[375]\ttraining's auc: 0.969936\tvalid_1's auc: 0.922904\n",
      "[376]\ttraining's auc: 0.970095\tvalid_1's auc: 0.923011\n",
      "[377]\ttraining's auc: 0.970146\tvalid_1's auc: 0.923074\n",
      "[378]\ttraining's auc: 0.97019\tvalid_1's auc: 0.923088\n",
      "[379]\ttraining's auc: 0.970246\tvalid_1's auc: 0.923103\n",
      "[380]\ttraining's auc: 0.970267\tvalid_1's auc: 0.923151\n",
      "[381]\ttraining's auc: 0.970383\tvalid_1's auc: 0.923172\n",
      "[382]\ttraining's auc: 0.970381\tvalid_1's auc: 0.923215\n",
      "[383]\ttraining's auc: 0.970405\tvalid_1's auc: 0.923282\n",
      "[384]\ttraining's auc: 0.970557\tvalid_1's auc: 0.923352\n",
      "[385]\ttraining's auc: 0.970644\tvalid_1's auc: 0.923437\n",
      "[386]\ttraining's auc: 0.970599\tvalid_1's auc: 0.923449\n",
      "[387]\ttraining's auc: 0.970579\tvalid_1's auc: 0.92345\n",
      "[388]\ttraining's auc: 0.970744\tvalid_1's auc: 0.923535\n",
      "[389]\ttraining's auc: 0.970857\tvalid_1's auc: 0.923631\n",
      "[390]\ttraining's auc: 0.970943\tvalid_1's auc: 0.923652\n",
      "[391]\ttraining's auc: 0.970971\tvalid_1's auc: 0.923701\n",
      "[392]\ttraining's auc: 0.971035\tvalid_1's auc: 0.923717\n",
      "[393]\ttraining's auc: 0.971041\tvalid_1's auc: 0.923738\n",
      "[394]\ttraining's auc: 0.9711\tvalid_1's auc: 0.923762\n",
      "[395]\ttraining's auc: 0.971178\tvalid_1's auc: 0.92382\n",
      "[396]\ttraining's auc: 0.971236\tvalid_1's auc: 0.923882\n",
      "[397]\ttraining's auc: 0.971348\tvalid_1's auc: 0.923963\n",
      "[398]\ttraining's auc: 0.971445\tvalid_1's auc: 0.924081\n",
      "[399]\ttraining's auc: 0.971574\tvalid_1's auc: 0.924175\n",
      "[400]\ttraining's auc: 0.97155\tvalid_1's auc: 0.924134\n",
      "[401]\ttraining's auc: 0.971554\tvalid_1's auc: 0.924166\n",
      "[402]\ttraining's auc: 0.971546\tvalid_1's auc: 0.924163\n",
      "[403]\ttraining's auc: 0.971637\tvalid_1's auc: 0.924207\n",
      "[404]\ttraining's auc: 0.971815\tvalid_1's auc: 0.924329\n",
      "[405]\ttraining's auc: 0.971995\tvalid_1's auc: 0.924475\n",
      "[406]\ttraining's auc: 0.971974\tvalid_1's auc: 0.924436\n",
      "[407]\ttraining's auc: 0.971948\tvalid_1's auc: 0.92448\n",
      "[408]\ttraining's auc: 0.97203\tvalid_1's auc: 0.924603\n",
      "[409]\ttraining's auc: 0.972216\tvalid_1's auc: 0.924692\n",
      "[410]\ttraining's auc: 0.972243\tvalid_1's auc: 0.924747\n",
      "[411]\ttraining's auc: 0.972357\tvalid_1's auc: 0.924839\n",
      "[412]\ttraining's auc: 0.97236\tvalid_1's auc: 0.924827\n",
      "[413]\ttraining's auc: 0.972361\tvalid_1's auc: 0.924855\n",
      "[414]\ttraining's auc: 0.972387\tvalid_1's auc: 0.924922\n",
      "[415]\ttraining's auc: 0.972376\tvalid_1's auc: 0.924883\n",
      "[416]\ttraining's auc: 0.972431\tvalid_1's auc: 0.924915\n",
      "[417]\ttraining's auc: 0.972512\tvalid_1's auc: 0.925011\n",
      "[418]\ttraining's auc: 0.972513\tvalid_1's auc: 0.924966\n",
      "[419]\ttraining's auc: 0.972653\tvalid_1's auc: 0.925022\n",
      "[420]\ttraining's auc: 0.972805\tvalid_1's auc: 0.925099\n",
      "[421]\ttraining's auc: 0.972887\tvalid_1's auc: 0.925155\n",
      "[422]\ttraining's auc: 0.973012\tvalid_1's auc: 0.925222\n",
      "[423]\ttraining's auc: 0.972996\tvalid_1's auc: 0.925231\n",
      "[424]\ttraining's auc: 0.973078\tvalid_1's auc: 0.925293\n",
      "[425]\ttraining's auc: 0.973128\tvalid_1's auc: 0.92534\n",
      "[426]\ttraining's auc: 0.973245\tvalid_1's auc: 0.925414\n",
      "[427]\ttraining's auc: 0.973267\tvalid_1's auc: 0.925346\n",
      "[428]\ttraining's auc: 0.97337\tvalid_1's auc: 0.925388\n",
      "[429]\ttraining's auc: 0.97342\tvalid_1's auc: 0.925402\n",
      "[430]\ttraining's auc: 0.973501\tvalid_1's auc: 0.925513\n",
      "[431]\ttraining's auc: 0.973596\tvalid_1's auc: 0.925538\n",
      "[432]\ttraining's auc: 0.973673\tvalid_1's auc: 0.92561\n",
      "[433]\ttraining's auc: 0.97367\tvalid_1's auc: 0.925586\n",
      "[434]\ttraining's auc: 0.973862\tvalid_1's auc: 0.925708\n",
      "[435]\ttraining's auc: 0.973892\tvalid_1's auc: 0.925765\n",
      "[436]\ttraining's auc: 0.974051\tvalid_1's auc: 0.92586\n",
      "[437]\ttraining's auc: 0.97416\tvalid_1's auc: 0.925949\n",
      "[438]\ttraining's auc: 0.974264\tvalid_1's auc: 0.926003\n",
      "[439]\ttraining's auc: 0.974286\tvalid_1's auc: 0.925988\n",
      "[440]\ttraining's auc: 0.974325\tvalid_1's auc: 0.925996\n",
      "[441]\ttraining's auc: 0.97446\tvalid_1's auc: 0.926055\n",
      "[442]\ttraining's auc: 0.974592\tvalid_1's auc: 0.926095\n",
      "[443]\ttraining's auc: 0.974706\tvalid_1's auc: 0.926209\n",
      "[444]\ttraining's auc: 0.974843\tvalid_1's auc: 0.926322\n",
      "[445]\ttraining's auc: 0.974923\tvalid_1's auc: 0.926367\n",
      "[446]\ttraining's auc: 0.974899\tvalid_1's auc: 0.926353\n",
      "[447]\ttraining's auc: 0.974909\tvalid_1's auc: 0.926352\n",
      "[448]\ttraining's auc: 0.974943\tvalid_1's auc: 0.92634\n",
      "[449]\ttraining's auc: 0.974924\tvalid_1's auc: 0.926317\n",
      "[450]\ttraining's auc: 0.975041\tvalid_1's auc: 0.926381\n",
      "[451]\ttraining's auc: 0.975131\tvalid_1's auc: 0.92646\n",
      "[452]\ttraining's auc: 0.975197\tvalid_1's auc: 0.926515\n",
      "[453]\ttraining's auc: 0.975223\tvalid_1's auc: 0.926528\n",
      "[454]\ttraining's auc: 0.975303\tvalid_1's auc: 0.926622\n",
      "[455]\ttraining's auc: 0.9753\tvalid_1's auc: 0.926652\n",
      "[456]\ttraining's auc: 0.975385\tvalid_1's auc: 0.92671\n",
      "[457]\ttraining's auc: 0.97541\tvalid_1's auc: 0.926709\n",
      "[458]\ttraining's auc: 0.975417\tvalid_1's auc: 0.926709\n",
      "[459]\ttraining's auc: 0.975459\tvalid_1's auc: 0.926738\n",
      "[460]\ttraining's auc: 0.975456\tvalid_1's auc: 0.926753\n",
      "[461]\ttraining's auc: 0.975568\tvalid_1's auc: 0.926766\n",
      "[462]\ttraining's auc: 0.975639\tvalid_1's auc: 0.92678\n",
      "[463]\ttraining's auc: 0.975731\tvalid_1's auc: 0.926844\n",
      "[464]\ttraining's auc: 0.97581\tvalid_1's auc: 0.926944\n",
      "[465]\ttraining's auc: 0.975928\tvalid_1's auc: 0.926974\n",
      "[466]\ttraining's auc: 0.976029\tvalid_1's auc: 0.927026\n",
      "[467]\ttraining's auc: 0.976006\tvalid_1's auc: 0.92703\n",
      "[468]\ttraining's auc: 0.976105\tvalid_1's auc: 0.927022\n",
      "[469]\ttraining's auc: 0.976197\tvalid_1's auc: 0.92706\n",
      "[470]\ttraining's auc: 0.976302\tvalid_1's auc: 0.927113\n",
      "[471]\ttraining's auc: 0.976373\tvalid_1's auc: 0.927098\n",
      "[472]\ttraining's auc: 0.976419\tvalid_1's auc: 0.927178\n",
      "[473]\ttraining's auc: 0.976438\tvalid_1's auc: 0.927208\n",
      "[474]\ttraining's auc: 0.976449\tvalid_1's auc: 0.927178\n",
      "[475]\ttraining's auc: 0.976567\tvalid_1's auc: 0.927266\n",
      "[476]\ttraining's auc: 0.976617\tvalid_1's auc: 0.927311\n",
      "[477]\ttraining's auc: 0.976685\tvalid_1's auc: 0.927351\n",
      "[478]\ttraining's auc: 0.97672\tvalid_1's auc: 0.927354\n",
      "[479]\ttraining's auc: 0.97677\tvalid_1's auc: 0.927386\n",
      "[480]\ttraining's auc: 0.976867\tvalid_1's auc: 0.927456\n",
      "[481]\ttraining's auc: 0.976952\tvalid_1's auc: 0.927508\n",
      "[482]\ttraining's auc: 0.977098\tvalid_1's auc: 0.927532\n",
      "[483]\ttraining's auc: 0.97723\tvalid_1's auc: 0.927627\n",
      "[484]\ttraining's auc: 0.977262\tvalid_1's auc: 0.927654\n",
      "[485]\ttraining's auc: 0.977341\tvalid_1's auc: 0.927692\n",
      "[486]\ttraining's auc: 0.977324\tvalid_1's auc: 0.927734\n",
      "[487]\ttraining's auc: 0.977429\tvalid_1's auc: 0.927714\n",
      "[488]\ttraining's auc: 0.977466\tvalid_1's auc: 0.927719\n",
      "[489]\ttraining's auc: 0.977497\tvalid_1's auc: 0.92773\n",
      "[490]\ttraining's auc: 0.977522\tvalid_1's auc: 0.927735\n",
      "[491]\ttraining's auc: 0.977549\tvalid_1's auc: 0.927726\n",
      "[492]\ttraining's auc: 0.977648\tvalid_1's auc: 0.927773\n",
      "[493]\ttraining's auc: 0.977726\tvalid_1's auc: 0.927744\n",
      "[494]\ttraining's auc: 0.977852\tvalid_1's auc: 0.92777\n",
      "[495]\ttraining's auc: 0.977942\tvalid_1's auc: 0.927818\n",
      "[496]\ttraining's auc: 0.97797\tvalid_1's auc: 0.927815\n",
      "[497]\ttraining's auc: 0.978088\tvalid_1's auc: 0.927884\n",
      "[498]\ttraining's auc: 0.978201\tvalid_1's auc: 0.927912\n",
      "[499]\ttraining's auc: 0.978255\tvalid_1's auc: 0.927959\n",
      "[500]\ttraining's auc: 0.978341\tvalid_1's auc: 0.928017\n",
      "[501]\ttraining's auc: 0.978405\tvalid_1's auc: 0.928059\n",
      "[502]\ttraining's auc: 0.978495\tvalid_1's auc: 0.928149\n",
      "[503]\ttraining's auc: 0.978551\tvalid_1's auc: 0.928228\n",
      "[504]\ttraining's auc: 0.978618\tvalid_1's auc: 0.928208\n",
      "[505]\ttraining's auc: 0.978666\tvalid_1's auc: 0.928273\n",
      "[506]\ttraining's auc: 0.978753\tvalid_1's auc: 0.928313\n",
      "[507]\ttraining's auc: 0.978864\tvalid_1's auc: 0.928382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[508]\ttraining's auc: 0.978987\tvalid_1's auc: 0.928444\n",
      "[509]\ttraining's auc: 0.979018\tvalid_1's auc: 0.928477\n",
      "[510]\ttraining's auc: 0.979111\tvalid_1's auc: 0.928553\n",
      "[511]\ttraining's auc: 0.979173\tvalid_1's auc: 0.928496\n",
      "[512]\ttraining's auc: 0.979216\tvalid_1's auc: 0.928533\n",
      "[513]\ttraining's auc: 0.979296\tvalid_1's auc: 0.928566\n",
      "[514]\ttraining's auc: 0.979332\tvalid_1's auc: 0.928602\n",
      "[515]\ttraining's auc: 0.979394\tvalid_1's auc: 0.928658\n",
      "[516]\ttraining's auc: 0.979378\tvalid_1's auc: 0.928674\n",
      "[517]\ttraining's auc: 0.979407\tvalid_1's auc: 0.928705\n",
      "[518]\ttraining's auc: 0.979476\tvalid_1's auc: 0.928774\n",
      "[519]\ttraining's auc: 0.97948\tvalid_1's auc: 0.928769\n",
      "[520]\ttraining's auc: 0.979572\tvalid_1's auc: 0.928833\n",
      "[521]\ttraining's auc: 0.979592\tvalid_1's auc: 0.928888\n",
      "[522]\ttraining's auc: 0.979606\tvalid_1's auc: 0.928906\n",
      "[523]\ttraining's auc: 0.979677\tvalid_1's auc: 0.928959\n",
      "[524]\ttraining's auc: 0.979747\tvalid_1's auc: 0.928981\n",
      "[525]\ttraining's auc: 0.979825\tvalid_1's auc: 0.928999\n",
      "[526]\ttraining's auc: 0.979885\tvalid_1's auc: 0.929041\n",
      "[527]\ttraining's auc: 0.979943\tvalid_1's auc: 0.929074\n",
      "[528]\ttraining's auc: 0.980004\tvalid_1's auc: 0.929059\n",
      "[529]\ttraining's auc: 0.98014\tvalid_1's auc: 0.929124\n",
      "[530]\ttraining's auc: 0.980146\tvalid_1's auc: 0.929094\n",
      "[531]\ttraining's auc: 0.980204\tvalid_1's auc: 0.929113\n",
      "[532]\ttraining's auc: 0.980251\tvalid_1's auc: 0.929119\n",
      "[533]\ttraining's auc: 0.980319\tvalid_1's auc: 0.929193\n",
      "[534]\ttraining's auc: 0.980418\tvalid_1's auc: 0.929214\n",
      "[535]\ttraining's auc: 0.980482\tvalid_1's auc: 0.929276\n",
      "[536]\ttraining's auc: 0.980495\tvalid_1's auc: 0.929252\n",
      "[537]\ttraining's auc: 0.980498\tvalid_1's auc: 0.929266\n",
      "[538]\ttraining's auc: 0.98056\tvalid_1's auc: 0.929312\n",
      "[539]\ttraining's auc: 0.980608\tvalid_1's auc: 0.929332\n",
      "[540]\ttraining's auc: 0.980648\tvalid_1's auc: 0.929374\n",
      "[541]\ttraining's auc: 0.980716\tvalid_1's auc: 0.92945\n",
      "[542]\ttraining's auc: 0.98074\tvalid_1's auc: 0.929445\n",
      "[543]\ttraining's auc: 0.980795\tvalid_1's auc: 0.929406\n",
      "[544]\ttraining's auc: 0.98084\tvalid_1's auc: 0.929415\n",
      "[545]\ttraining's auc: 0.980884\tvalid_1's auc: 0.929424\n",
      "[546]\ttraining's auc: 0.980924\tvalid_1's auc: 0.929458\n",
      "[547]\ttraining's auc: 0.980971\tvalid_1's auc: 0.929502\n",
      "[548]\ttraining's auc: 0.981027\tvalid_1's auc: 0.929504\n",
      "[549]\ttraining's auc: 0.98108\tvalid_1's auc: 0.929592\n",
      "[550]\ttraining's auc: 0.981087\tvalid_1's auc: 0.929609\n",
      "[551]\ttraining's auc: 0.98117\tvalid_1's auc: 0.929658\n",
      "[552]\ttraining's auc: 0.981241\tvalid_1's auc: 0.929697\n",
      "[553]\ttraining's auc: 0.98133\tvalid_1's auc: 0.929709\n",
      "[554]\ttraining's auc: 0.981339\tvalid_1's auc: 0.929728\n",
      "[555]\ttraining's auc: 0.981396\tvalid_1's auc: 0.929716\n",
      "[556]\ttraining's auc: 0.9814\tvalid_1's auc: 0.929717\n",
      "[557]\ttraining's auc: 0.981454\tvalid_1's auc: 0.929695\n",
      "[558]\ttraining's auc: 0.981499\tvalid_1's auc: 0.929705\n",
      "[559]\ttraining's auc: 0.981568\tvalid_1's auc: 0.929716\n",
      "[560]\ttraining's auc: 0.981629\tvalid_1's auc: 0.929721\n",
      "[561]\ttraining's auc: 0.981685\tvalid_1's auc: 0.929732\n",
      "[562]\ttraining's auc: 0.981743\tvalid_1's auc: 0.929739\n",
      "[563]\ttraining's auc: 0.981776\tvalid_1's auc: 0.929748\n",
      "[564]\ttraining's auc: 0.981756\tvalid_1's auc: 0.929749\n",
      "[565]\ttraining's auc: 0.98183\tvalid_1's auc: 0.929765\n",
      "[566]\ttraining's auc: 0.98187\tvalid_1's auc: 0.92975\n",
      "[567]\ttraining's auc: 0.981916\tvalid_1's auc: 0.929789\n",
      "[568]\ttraining's auc: 0.981967\tvalid_1's auc: 0.929845\n",
      "[569]\ttraining's auc: 0.982017\tvalid_1's auc: 0.929855\n",
      "[570]\ttraining's auc: 0.982104\tvalid_1's auc: 0.929919\n",
      "[571]\ttraining's auc: 0.982155\tvalid_1's auc: 0.929882\n",
      "[572]\ttraining's auc: 0.982215\tvalid_1's auc: 0.929823\n",
      "[573]\ttraining's auc: 0.982273\tvalid_1's auc: 0.929795\n",
      "[574]\ttraining's auc: 0.982363\tvalid_1's auc: 0.929847\n",
      "[575]\ttraining's auc: 0.982453\tvalid_1's auc: 0.929894\n",
      "[576]\ttraining's auc: 0.982472\tvalid_1's auc: 0.929942\n",
      "[577]\ttraining's auc: 0.982525\tvalid_1's auc: 0.930001\n",
      "[578]\ttraining's auc: 0.982586\tvalid_1's auc: 0.930047\n",
      "[579]\ttraining's auc: 0.982641\tvalid_1's auc: 0.930089\n",
      "[580]\ttraining's auc: 0.982705\tvalid_1's auc: 0.930134\n",
      "[581]\ttraining's auc: 0.982723\tvalid_1's auc: 0.930171\n",
      "[582]\ttraining's auc: 0.982769\tvalid_1's auc: 0.93017\n",
      "[583]\ttraining's auc: 0.982834\tvalid_1's auc: 0.93015\n",
      "[584]\ttraining's auc: 0.98284\tvalid_1's auc: 0.930139\n",
      "[585]\ttraining's auc: 0.98289\tvalid_1's auc: 0.930182\n",
      "[586]\ttraining's auc: 0.982951\tvalid_1's auc: 0.930255\n",
      "[587]\ttraining's auc: 0.982966\tvalid_1's auc: 0.93027\n",
      "[588]\ttraining's auc: 0.983001\tvalid_1's auc: 0.930271\n",
      "[589]\ttraining's auc: 0.98303\tvalid_1's auc: 0.93022\n",
      "[590]\ttraining's auc: 0.98306\tvalid_1's auc: 0.930208\n",
      "[591]\ttraining's auc: 0.983122\tvalid_1's auc: 0.930283\n",
      "[592]\ttraining's auc: 0.983202\tvalid_1's auc: 0.930274\n",
      "[593]\ttraining's auc: 0.983241\tvalid_1's auc: 0.930318\n",
      "[594]\ttraining's auc: 0.983272\tvalid_1's auc: 0.93032\n",
      "[595]\ttraining's auc: 0.983347\tvalid_1's auc: 0.930326\n",
      "[596]\ttraining's auc: 0.9834\tvalid_1's auc: 0.930323\n",
      "[597]\ttraining's auc: 0.983472\tvalid_1's auc: 0.930373\n",
      "[598]\ttraining's auc: 0.983528\tvalid_1's auc: 0.930462\n",
      "[599]\ttraining's auc: 0.983562\tvalid_1's auc: 0.930489\n",
      "[600]\ttraining's auc: 0.983602\tvalid_1's auc: 0.930539\n",
      "[601]\ttraining's auc: 0.983633\tvalid_1's auc: 0.930594\n",
      "[602]\ttraining's auc: 0.983684\tvalid_1's auc: 0.930604\n",
      "[603]\ttraining's auc: 0.983749\tvalid_1's auc: 0.930632\n",
      "[604]\ttraining's auc: 0.983773\tvalid_1's auc: 0.930694\n",
      "[605]\ttraining's auc: 0.983799\tvalid_1's auc: 0.930719\n",
      "[606]\ttraining's auc: 0.983806\tvalid_1's auc: 0.930738\n",
      "[607]\ttraining's auc: 0.983812\tvalid_1's auc: 0.930781\n",
      "[608]\ttraining's auc: 0.983815\tvalid_1's auc: 0.930748\n",
      "[609]\ttraining's auc: 0.983819\tvalid_1's auc: 0.93077\n",
      "[610]\ttraining's auc: 0.983824\tvalid_1's auc: 0.930763\n",
      "[611]\ttraining's auc: 0.983863\tvalid_1's auc: 0.93075\n",
      "[612]\ttraining's auc: 0.983839\tvalid_1's auc: 0.930749\n",
      "[613]\ttraining's auc: 0.983905\tvalid_1's auc: 0.930743\n",
      "[614]\ttraining's auc: 0.983948\tvalid_1's auc: 0.930764\n",
      "[615]\ttraining's auc: 0.983977\tvalid_1's auc: 0.930799\n",
      "[616]\ttraining's auc: 0.984009\tvalid_1's auc: 0.930778\n",
      "[617]\ttraining's auc: 0.98407\tvalid_1's auc: 0.930808\n",
      "[618]\ttraining's auc: 0.9841\tvalid_1's auc: 0.930792\n",
      "[619]\ttraining's auc: 0.984108\tvalid_1's auc: 0.930764\n",
      "[620]\ttraining's auc: 0.984161\tvalid_1's auc: 0.930842\n",
      "[621]\ttraining's auc: 0.984185\tvalid_1's auc: 0.930864\n",
      "[622]\ttraining's auc: 0.984217\tvalid_1's auc: 0.930892\n",
      "[623]\ttraining's auc: 0.984234\tvalid_1's auc: 0.930905\n",
      "[624]\ttraining's auc: 0.984261\tvalid_1's auc: 0.930904\n",
      "[625]\ttraining's auc: 0.984302\tvalid_1's auc: 0.930949\n",
      "[626]\ttraining's auc: 0.984336\tvalid_1's auc: 0.930958\n",
      "[627]\ttraining's auc: 0.984345\tvalid_1's auc: 0.93094\n",
      "[628]\ttraining's auc: 0.984415\tvalid_1's auc: 0.93099\n",
      "[629]\ttraining's auc: 0.984476\tvalid_1's auc: 0.931036\n",
      "[630]\ttraining's auc: 0.984534\tvalid_1's auc: 0.931027\n",
      "[631]\ttraining's auc: 0.984569\tvalid_1's auc: 0.931084\n",
      "[632]\ttraining's auc: 0.984612\tvalid_1's auc: 0.931098\n",
      "[633]\ttraining's auc: 0.98461\tvalid_1's auc: 0.931037\n",
      "[634]\ttraining's auc: 0.98463\tvalid_1's auc: 0.931\n",
      "[635]\ttraining's auc: 0.984652\tvalid_1's auc: 0.930963\n",
      "[636]\ttraining's auc: 0.984674\tvalid_1's auc: 0.930967\n",
      "[637]\ttraining's auc: 0.984693\tvalid_1's auc: 0.93098\n",
      "[638]\ttraining's auc: 0.984692\tvalid_1's auc: 0.931011\n",
      "[639]\ttraining's auc: 0.984733\tvalid_1's auc: 0.930981\n",
      "[640]\ttraining's auc: 0.98476\tvalid_1's auc: 0.930992\n",
      "[641]\ttraining's auc: 0.984805\tvalid_1's auc: 0.931034\n",
      "[642]\ttraining's auc: 0.984839\tvalid_1's auc: 0.930983\n",
      "[643]\ttraining's auc: 0.984882\tvalid_1's auc: 0.931009\n",
      "[644]\ttraining's auc: 0.984887\tvalid_1's auc: 0.931003\n",
      "[645]\ttraining's auc: 0.984934\tvalid_1's auc: 0.931085\n",
      "[646]\ttraining's auc: 0.984971\tvalid_1's auc: 0.931049\n",
      "[647]\ttraining's auc: 0.985017\tvalid_1's auc: 0.93105\n",
      "[648]\ttraining's auc: 0.985045\tvalid_1's auc: 0.931005\n",
      "[649]\ttraining's auc: 0.985093\tvalid_1's auc: 0.93102\n",
      "[650]\ttraining's auc: 0.985114\tvalid_1's auc: 0.930956\n",
      "[651]\ttraining's auc: 0.985139\tvalid_1's auc: 0.930957\n",
      "[652]\ttraining's auc: 0.985197\tvalid_1's auc: 0.930952\n",
      "[653]\ttraining's auc: 0.985234\tvalid_1's auc: 0.930965\n",
      "[654]\ttraining's auc: 0.985284\tvalid_1's auc: 0.931043\n",
      "[655]\ttraining's auc: 0.985272\tvalid_1's auc: 0.931046\n",
      "[656]\ttraining's auc: 0.985325\tvalid_1's auc: 0.931025\n",
      "[657]\ttraining's auc: 0.985366\tvalid_1's auc: 0.930988\n",
      "[658]\ttraining's auc: 0.985389\tvalid_1's auc: 0.930958\n",
      "[659]\ttraining's auc: 0.985398\tvalid_1's auc: 0.930972\n",
      "[660]\ttraining's auc: 0.98542\tvalid_1's auc: 0.930969\n",
      "[661]\ttraining's auc: 0.985474\tvalid_1's auc: 0.931021\n",
      "[662]\ttraining's auc: 0.985514\tvalid_1's auc: 0.931041\n",
      "[663]\ttraining's auc: 0.985548\tvalid_1's auc: 0.931009\n",
      "[664]\ttraining's auc: 0.985578\tvalid_1's auc: 0.930999\n",
      "[665]\ttraining's auc: 0.985597\tvalid_1's auc: 0.930986\n",
      "[666]\ttraining's auc: 0.985618\tvalid_1's auc: 0.930959\n",
      "[667]\ttraining's auc: 0.985642\tvalid_1's auc: 0.930926\n",
      "[668]\ttraining's auc: 0.985682\tvalid_1's auc: 0.930904\n",
      "[669]\ttraining's auc: 0.985736\tvalid_1's auc: 0.930949\n",
      "[670]\ttraining's auc: 0.985776\tvalid_1's auc: 0.930956\n",
      "[671]\ttraining's auc: 0.9858\tvalid_1's auc: 0.930987\n",
      "[672]\ttraining's auc: 0.985841\tvalid_1's auc: 0.931034\n",
      "[673]\ttraining's auc: 0.98586\tvalid_1's auc: 0.930985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[674]\ttraining's auc: 0.985878\tvalid_1's auc: 0.931003\n",
      "[675]\ttraining's auc: 0.985891\tvalid_1's auc: 0.931003\n",
      "[676]\ttraining's auc: 0.985933\tvalid_1's auc: 0.931022\n",
      "[677]\ttraining's auc: 0.985954\tvalid_1's auc: 0.931015\n",
      "[678]\ttraining's auc: 0.985986\tvalid_1's auc: 0.931004\n",
      "[679]\ttraining's auc: 0.98601\tvalid_1's auc: 0.930992\n",
      "[680]\ttraining's auc: 0.986023\tvalid_1's auc: 0.931008\n",
      "[681]\ttraining's auc: 0.986052\tvalid_1's auc: 0.93103\n",
      "[682]\ttraining's auc: 0.986082\tvalid_1's auc: 0.931007\n",
      "Early stopping, best iteration is:\n",
      "[632]\ttraining's auc: 0.984612\tvalid_1's auc: 0.931098\n"
     ]
    }
   ],
   "source": [
    "classifier = lgb.train(param, train_set,num_boost_round=100, \n",
    "                valid_sets = [train_set, test_set], \n",
    "                early_stopping_rounds=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The penalty for mislabeling a loan default as legitimate is having a organize's money stolen, which the credit card company typically reimburses. To address this issue we need to protect the companyâ€™s finances by trying to flag as many loan defaults(No matter the number of loan applications rejected, there will still be debtors that default)  \n",
    "Therefore, AUC is a good measure to determine, when the costs of False Positive is high.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
